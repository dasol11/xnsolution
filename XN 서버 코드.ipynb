{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2677e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix,f1_score,roc_curve\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Logistic Regression\n",
    "def logisticRegression(trainX, trainY, testX, testY, decision_boundary=0.5):\n",
    "    '''\n",
    "        @Input\n",
    "            trainX : type : array\n",
    "            trainY : type : array\n",
    "            testX : type : array\n",
    "            testY : type : array\n",
    "\n",
    "            decision_boundary : type : float (0~1사이의 값)\n",
    "        @Output\n",
    "            AucScore : type : float 64\n",
    "            f1 : type : float 64\n",
    "            precision : type : float 64\n",
    "            recall : type : float 64\n",
    "            confusionmatrix : array\n",
    "    '''\n",
    "    model = LogisticRegression(random_state=0).fit(trainX, trainY)\n",
    "\n",
    "    # Logistic Regression 모델 Pickle 파일로 저장\n",
    "\n",
    "    saveModel = joblib.dump(model, 'Logistic.pkl')\n",
    "\n",
    "    anomalyScore = model.predict_proba(testX)\n",
    "    #[ [1, 0]  ]\n",
    "    pred = np.array([], dtype=np.int32)\n",
    "\n",
    "    for i in range(len(anomalyScore)):\n",
    "        if (anomalyScore[i][1] > decision_boundary) == True:\n",
    "            pred = np.append(pred, 1)\n",
    "        else:\n",
    "            pred = np.append(pred, 0)\n",
    "\n",
    "    cofMat = confusion_matrix(testY, pred, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    tpr = cofMat[1, 1] / (cofMat[1, 1] + cofMat[1, 0])\n",
    "    fpr = cofMat[0, 1] / (cofMat[0, 0] + cofMat[0, 1])\n",
    "    \n",
    "    fpr_array = np.array([], dtype=np.int32)\n",
    "    tpr_array = np.array([], dtype=np.int32)\n",
    "\n",
    "    # Decision Boundary 별 FPR, TPR 계산 (AUC Score 시각화 용도 )\n",
    "    for i in range(1, 100):\n",
    "        predd = np.array([], dtype=np.int32)\n",
    "        for j in range(len(anomalyScore)):\n",
    "            decision_boundary = i / 100\n",
    "            if (anomalyScore[j][1] > decision_boundary) == True:\n",
    "                predd = np.append(predd, 1)\n",
    "            else:\n",
    "                predd = np.append(predd, 0)\n",
    "        cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "        \n",
    "        ##confusion matrix의 각 요소\n",
    "        auc_tpr = cofMatt[1, 1] / (cofMatt[1, 1] + cofMatt[1, 0])\n",
    "        auc_fpr = cofMatt[0, 1] / (cofMatt[0, 0] + cofMatt[0, 1])\n",
    "        fpr_array = np.append(fpr_array, auc_fpr)\n",
    "        tpr_array = np.append(tpr_array, auc_tpr)\n",
    "\n",
    "    # calculate AUC of model\n",
    "    auc = roc_auc_score(testY, pred)\n",
    "    return ({\"AnomalyScore\": np.round(anomalyScore[:,1],3),\"Prediction\":pred ,\"Aucscore\": str(auc), \"Fpr\": fpr, \"Tpr\": tpr, \"Fprarray\":fpr_array, \"Tprarray\":tpr_array})\n",
    "              # \"accuracy\": str(accuracy), \"precision\": str(precision), \"recall\": str(recall), \"f1\": str(f1), \"tp\": str(tp), \"fp\": str(fp), \"fn\": str(fn), \"Predarray\" :predarray,\n",
    "              # \"tn\": str(tn)})  # type : int32\n",
    "\n",
    "def classification_model_load_logistic(model_wd, testX, decision_boundary=0.5):\n",
    "    model_wd = 'Logistic.pkl'\n",
    "    model = joblib.load(model_wd)\n",
    "    anomalyScore = model.predict_proba(testX)\n",
    "    pred = np.array([], dtype=np.int32)\n",
    "    \n",
    "    for i in range(len(anomalyScore)):\n",
    "        if (anomalyScore[i][1] > decision_boundary) == True:\n",
    "            pred = np.append(pred, 1)\n",
    "        else:\n",
    "            pred = np.append(pred, 0)\n",
    "\n",
    "    return {\"AnomalyScore\": np.round(anomalyScore,3), \"Prediction\":pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a10211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff575e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspace/xnsolution/data/1_Iris_virginica.csv', encoding='euc-kr')\n",
    "\n",
    "df['label'][np.where(df['label'] == 'outlier')[0]] = 1\n",
    "df['label'][np.where(df['label'] == 'target')[0]] = 0\n",
    "\n",
    "X = df.iloc[:,0:4]\n",
    "y = df.iloc[:,4]\n",
    "y= y.astype('category')\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X,y, test_size=0.3)\n",
    "\n",
    "# model = classification_model_load_logistic(model, testX, decision_boundary=0.5)\n",
    "# load_model = classification_model_load_logistic('Logistic.pkl', testX, decision_boundary=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae210af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as pydatetime\n",
    "import os\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix,f1_score,roc_curve\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "    def classification_decisiontree(trainX, testX, trainY, testY, criterion='gini', max_depth=None, min_samples_leaf=1):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainX : array\n",
    "            Train X.\n",
    "        testX : array\n",
    "            Test X.\n",
    "        trainY : array\n",
    "            Train Y.\n",
    "        testY : array\n",
    "            Test Y.\n",
    "        criterion : {'gini', 'entropy', 'log_loss'}, default = \"gini\"\n",
    "            의사 결정 나무의 분기 기준.\n",
    "        max_depth : int, optional, default = None\n",
    "            의사 결정 나무의 최대 깊이. 의사 결정 나무가 최대 깊이에 도달하면 더 이상 분기하지 않음.\n",
    "        min_samples_leaf : int or float, optional, default = 2\n",
    "            노드 별 최소 샘플 갯수, 터미널 노드안에 샘플 수가 min_samples_split과 같아지면 더 이상 분기하지 않음.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model :\n",
    "            분류 의사결정나무 모델.\n",
    "        confusionMatrix : array\n",
    "            분류 성능 평가를 위한 Confusion Matrix\n",
    "        fpr : array\n",
    "            False Positive Rate\n",
    "        tpr : array\n",
    "            True Positive Rate\n",
    "        auc_score : float\n",
    "            fpr, tpr의 AUC(Area Under Cruve) scroe (0~1). 1에 가까울 수록 분류 성능이 좋음을 의미함\n",
    "        accuracy : float\n",
    "            accuracy.\n",
    "        precision : float\n",
    "            precision\n",
    "        recall : float\n",
    "            recall\n",
    "        f1 : float\n",
    "            f1 score\n",
    "        tp : int\n",
    "            True positive\n",
    "        fp : int\n",
    "            False positive\n",
    "        fn : int\n",
    "            False negative\n",
    "        tn : int\n",
    "            True negative\n",
    "        \"\"\"\n",
    "\n",
    "        trainnX = trainX\n",
    "\n",
    "        if isinstance(trainX, pd.DataFrame):\n",
    "            trainX = trainX.values\n",
    "        if isinstance(testX, pd.DataFrame):\n",
    "            testX = testX.values\n",
    "        if isinstance(trainY, pd.DataFrame):\n",
    "            trainY = trainY.values\n",
    "        if isinstance(testY, pd.DataFrame):\n",
    "            testY = testY.values\n",
    "\n",
    "        # Decision Tree Model\n",
    "        model = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        model.fit(trainX, trainY)\n",
    "\n",
    "        # Decision Tree 모델 Pickle 파일로 저장\n",
    "        modelfilename = Cmm.GetSaveModelName(\"CFDecisionTree\")\n",
    "        modelpath = Cmm.GetSaveModelPath() + modelfilename\n",
    "        saveModel = joblib.dump(model, modelpath)\n",
    "        modelstr64 = Cmm.FileToBase64(modelpath)\n",
    "\n",
    "        if os.path.exists(modelpath):\n",
    "            os.remove(modelpath)\n",
    "\n",
    "        # 예측\n",
    "        anomalyScore = model.predict_proba(testX)  # 예측 분류 값\n",
    "        pred = model.predict(testX)\n",
    "\n",
    "        predarray = Cmm.ListToOneValueArraystring(pred)\n",
    "\n",
    "        # 분류 평가 지표\n",
    "        cofMat = confusion_matrix(testY, pred, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "        # confusion matrix의 각 요소\n",
    "        tp = cofMat[1, 1]\n",
    "        tn = cofMat[0, 0]\n",
    "        fn = cofMat[1, 0]\n",
    "        fp = cofMat[0, 1]\n",
    "\n",
    "        accuracy = accuracy_score(testY, pred)\n",
    "        precision = precision_score(testY, pred)\n",
    "        recall = recall_score(testY, pred)\n",
    "        f1 = f1_score(testY, pred)\n",
    "\n",
    "        # calculate AUC of model\n",
    "        auc = roc_auc_score(testY, pred)\n",
    "\n",
    "\n",
    "        fpr_array = np.array([], dtype=np.int32)\n",
    "        tpr_array = np.array([], dtype=np.int32)\n",
    "        for i in range(1, 100):\n",
    "            predd = np.array([], dtype=np.int32)\n",
    "            for j in range(len(anomalyScore)):\n",
    "                decision_boundary = i / 100\n",
    "                if (anomalyScore[j][1] > decision_boundary) == True:\n",
    "                    predd = np.append(predd, 1)\n",
    "                else:\n",
    "                    predd = np.append(predd, 0)\n",
    "\n",
    "            cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "            ##confusion matrix의 각 요소\n",
    "            tpp = cofMatt[1, 1]\n",
    "            tnn = cofMatt[0, 0]\n",
    "            fnn = cofMatt[1, 0]\n",
    "            fpp = cofMatt[0, 1]\n",
    "            tpr = tpp / (tpp + fnn)\n",
    "            fpr = fpp / (tnn + fpp)\n",
    "\n",
    "            fpr_array = np.append(fpr_array, fpr)\n",
    "            tpr_array = np.append(tpr_array, tpr)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "\n",
    "        lists = fpr.tolist()\n",
    "        fprJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = tpr.tolist()\n",
    "        tprJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        lists = cofMat.tolist()\n",
    "        cofMatJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = fpr_array.tolist()\n",
    "        fpr_arrayJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = tpr_array.tolist()\n",
    "        tpr_arrayJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        anomalyScore = np.round(anomalyScore[:, 1], 3)\n",
    "        lists = anomalyScore.tolist()\n",
    "        anomalyScoreJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        plt.rc('font', family='Malgun Gothic')\n",
    "        if max_depth == None:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "        else:\n",
    "            plt.figure(figsize=(4 * max_depth, 2 * max_depth))\n",
    "        tree.plot_tree(model,\n",
    "                       feature_names=trainnX.columns,\n",
    "                       impurity=True, filled=True,\n",
    "                       rounded=True, max_depth=max_depth)\n",
    "\n",
    "        fname = ('C:/Temp/' + str(pydatetime.datetime.now().timestamp()) + \".png\")\n",
    "        plt.savefig(fname, dpi=100)\n",
    "        str64 = Cmm.FileToBase64(fname)\n",
    "\n",
    "        if os.path.exists(fname):\n",
    "            os.remove(fname)\n",
    "\n",
    "\n",
    "        return {\"AnomalyScore\":anomalyScoreJson, \"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson,\"Fprarray\": fpr_arrayJson, \"Tprarray\": tpr_arrayJson, \"Predarray\" :predarray}, {\"Value\" :str(str64),\"Modelfile\": str(modelfilename), \"Modelcontents\": str(modelstr64)}\n",
    "                 # \"model\": str(model),  \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "                 # \"tn\": tn})  # type : int32\n",
    "\n",
    "def classification_model_load_DT(model, testX):\n",
    "    anomalyScore = model.predict_proba(testX)  # 예측 분류 값\n",
    "    pred = model.predict(testX)\n",
    "\n",
    "    predarray = Cmm.ListToOneValueArraystring(pred)\n",
    "\n",
    "    # # 분류 평가 지표\n",
    "    # cofMat = confusion_matrix(testY, pred, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    # # confusion matrix의 각 요소\n",
    "    # tp = cofMat[1, 1]\n",
    "    # tn = cofMat[0, 0]\n",
    "    # fn = cofMat[1, 0]\n",
    "    # fp = cofMat[0, 1]\n",
    "    #\n",
    "    # accuracy = accuracy_score(testY, pred)\n",
    "    # precision = precision_score(testY, pred)\n",
    "    # recall = recall_score(testY, pred)\n",
    "    # f1 = f1_score(testY, pred)\n",
    "    #\n",
    "    # # calculate AUC of model\n",
    "    # auc = roc_auc_score(testY, pred)\n",
    "    #\n",
    "    # fpr_array = np.array([], dtype=np.int32)\n",
    "    # tpr_array = np.array([], dtype=np.int32)\n",
    "    # for i in range(1, 100):\n",
    "    #     predd = np.array([], dtype=np.int32)\n",
    "    #     for j in range(len(pred_prob)):\n",
    "    #         decision_boundary = i / 100\n",
    "    #         if (pred_prob[j][1] > decision_boundary) == True:\n",
    "    #             predd = np.append(predd, 1)\n",
    "    #         else:\n",
    "    #             predd = np.append(predd, 0)\n",
    "    #\n",
    "    #     cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    #     ##confusion matrix의 각 요소\n",
    "    #     tpp = cofMatt[1, 1]\n",
    "    #     tnn = cofMatt[0, 0]\n",
    "    #     fnn = cofMatt[1, 0]\n",
    "    #     fpp = cofMatt[0, 1]\n",
    "    #     tpr = tpp / (tpp + fnn)\n",
    "    #     fpr = fpp / (tnn + fpp)\n",
    "    #\n",
    "    #     fpr_array = np.append(fpr_array, fpr)\n",
    "    #     tpr_array = np.append(tpr_array, tpr)\n",
    "    #\n",
    "    # fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "    #\n",
    "    # lists = fpr.tolist()\n",
    "    # fprJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = tpr.tolist()\n",
    "    # tprJson = Cmm.NumlistToArraystring(lists)\n",
    "    #\n",
    "    # lists = cofMat.tolist()\n",
    "    # cofMatJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = fpr_array.tolist()\n",
    "    # fpr_arrayJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = tpr_array.tolist()\n",
    "    # tpr_arrayJson = Cmm.NumlistToArraystring(lists)\n",
    "    #\n",
    "    # plt.rc('font', family='Malgun Gothic')\n",
    "    # if max_depth == None:\n",
    "    #     plt.figure(figsize=(10, 8))\n",
    "    # else:\n",
    "    #     plt.figure(figsize=(4 * max_depth, 2 * max_depth))\n",
    "    # tree.plot_tree(model,\n",
    "    #                feature_names=trainnX.columns,\n",
    "    #                impurity=True, filled=True,\n",
    "    #                rounded=True, max_depth=max_depth)\n",
    "    #\n",
    "    # fname = ('C:/Temp/' + str(pydatetime.datetime.now().timestamp()) + \".png\")\n",
    "    # plt.savefig(fname, dpi=100)\n",
    "    # str64 = Cmm.FileToBase64(fname)\n",
    "    #\n",
    "    # if os.path.exists(fname):\n",
    "    #     os.remove(fname)\n",
    "\n",
    "    return {\"Predarray\": predarray}\n",
    "    # return {\"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\": fpr_arrayJson, \"Tprarray\": tpr_arrayJson,\n",
    "    #         \"Predarray\": predarray}, {\"Value\": str(str64), \"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "    # \"model\": str(model),  \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "    # \"tn\": tn})  # type : int32\n",
    "\n",
    "# a = Classification.decisiontreeexcute('SET2022000007-CS','SET2022000007-CT', '계량완료위치,사출최대속도','판정코드','gini',3,2)\n",
    "# print(a)\n",
    "\n",
    "# ##예제\n",
    "# # import pandas as pd\n",
    "# ab_tr = pd.read_csv('C:\\\\Classfication\\\\ab_tr.csv')\n",
    "# ab_te = pd.read_csv('C:\\\\Classfication\\\\ab_te.csv')\n",
    "# # #\n",
    "# # #ab_tr = ab_tr.drop(['Unnamed: 0'],axis=1)\n",
    "# # #ab_te = ab_te.drop(['Unnamed: 0'],axis=1)\n",
    "# # #\n",
    "# ab_tr_x = ab_tr.drop(['target'],axis=1)\n",
    "# ab_te_x = ab_te.drop(['target'],axis=1)\n",
    "# print(ab_tr_x)\n",
    "# print(ab_te_x)\n",
    "# ab_tr_y = ab_tr['target']\n",
    "# ab_te_y = ab_te['target']\n",
    "# #trainX = ab_tr_x\n",
    "# #trainY = ab_tr_y\n",
    "#\n",
    "# #testX = ab_te_x\n",
    "# #testY = ab_te_y\n",
    "#\n",
    "# # # # print(ab_tr_y)\n",
    "# # # # print(ab_te_y)\n",
    "# #\n",
    "# ab_dt = classification_decisiontree(ab_tr_x, ab_te_x, ab_tr_y, ab_te_y,max_depth=3)\n",
    "# print(ab_dt)\n",
    "\n",
    "\n",
    "#DT_model = joblib.load()\n",
    "#print(classification_model_load_DT(DT_model,ab_te_x,ab_te_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from XNCmmLib import XNCmmUnit as Cmm\n",
    "import XNDatabaseLib as DbLib\n",
    "import os\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix,f1_score,roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import joblib\n",
    "\n",
    "class Classification:\n",
    "\n",
    "    def knnexcute(trdataID, tsdataID, cols, fcols,n_neighbors):\n",
    "        # TrainingID의 cols열을 dataframe 변수\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(trdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        trdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(trdf)\n",
    "        # 예측하는 데이터열을 dataframe 변수\n",
    "        trfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(trfdf)\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(tsdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        tsdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(tsdf)\n",
    "        tsfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(tsfdf)\n",
    "        # cbm 호출\n",
    "        a = Classification.knnClassifier(trdf, trfdf ,tsdf, tsfdf, n_neighbors)\n",
    "        jstr = Cmm.ClassToJson(a)\n",
    "\n",
    "        return jstr\n",
    "\n",
    "    # KNN (Classification)\n",
    "    def knnClassifier(x_train, y_train, x_test, y_test, number_neighbors):\n",
    "        '''\n",
    "        @Input\n",
    "            x_train : type : array\n",
    "            y_train : type : array\n",
    "            x_test : type : array\n",
    "            n_neighbors : type : int\n",
    "        @Output\n",
    "            auc_score : type : float64\n",
    "            fpr : type : array\n",
    "            fpr : type : array\n",
    "        '''\n",
    "\n",
    "        if isinstance(x_train, pd.DataFrame):\n",
    "            x_train = x_train.values\n",
    "        if isinstance(x_test, pd.DataFrame):\n",
    "            x_test = x_test.values\n",
    "        if isinstance(y_train, pd.DataFrame):\n",
    "            y_train = y_train.values\n",
    "        if isinstance(y_test, pd.DataFrame):\n",
    "            y_test = y_test.values\n",
    "\n",
    "        model = KNeighborsClassifier(n_neighbors=number_neighbors)\n",
    "        model.fit(x_train, y_train)\n",
    "        anomalyScore = model.predict_proba(x_test)\n",
    "\n",
    "        # # KNN 모델 Pickle 파일로 저장\n",
    "        modelfilename = Cmm.GetSaveModelName(\"CFKnn\")\n",
    "        modelpath = Cmm.GetSaveModelPath() + modelfilename\n",
    "        saveModel = joblib.dump(model, modelpath)\n",
    "        str64 = Cmm.FileToBase64(modelpath)\n",
    "\n",
    "        if os.path.exists(modelpath):\n",
    "            os.remove(modelpath)\n",
    "\n",
    "\n",
    "        y_pred = model.predict(x_test)  # 불량 : 1, 정상 : 0\n",
    "\n",
    "        predarray = Cmm.ListToOneValueArraystring(y_pred)\n",
    "\n",
    "        # calculate AUC of model\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        fpr_array = np.array([], dtype=np.int32)\n",
    "        tpr_array = np.array([], dtype=np.int32)\n",
    "\n",
    "        for i in range(1, 100):\n",
    "            predd = np.array([], dtype=np.int32)\n",
    "            for j in range(len(anomalyScore)):\n",
    "                decision_boundary = i / 100\n",
    "                if (anomalyScore[j][1] > decision_boundary) == True:\n",
    "                    predd = np.append(predd, 1)\n",
    "                else:\n",
    "                    predd = np.append(predd, 0)\n",
    "\n",
    "            cofMatt = confusion_matrix(y_test, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "            ##confusion matrix의 각 요소\n",
    "            tpp = cofMatt[1, 1]\n",
    "            tnn = cofMatt[0, 0]\n",
    "            fnn = cofMatt[1, 0]\n",
    "            fpp = cofMatt[0, 1]\n",
    "            tpr = tpp / (tpp + fnn)\n",
    "            fpr = fpp / (tnn + fpp)\n",
    "\n",
    "            fpr_array = np.append(fpr_array, fpr)\n",
    "            tpr_array = np.append(tpr_array, tpr)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "        lists = fpr.tolist()\n",
    "        fprJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = tpr.tolist()\n",
    "        tprJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        anomalyScore = np.round(anomalyScore[:, 1], 3)\n",
    "        lists = anomalyScore.tolist()\n",
    "        anomalyScoreJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        fpr_array = Cmm.ListToOneValueArraystring(fpr_array)\n",
    "        tpr_array = Cmm.ListToOneValueArraystring(tpr_array)\n",
    "\n",
    "\n",
    "        return ({\"AnomalyScore\" : anomalyScoreJson, \"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\":fpr_array, \"Tprarray\":tpr_array, \"Predarray\" : predarray}), {\"Modelfile\":str(modelfilename),\"Modelcontents\": str(str64)}\n",
    "\n",
    "def classification_model_load_KN(model, x_test):\n",
    "    anomalyScore = model.predict_proba(x_test)\n",
    "    y_pred = model.predict(x_test)  # 불량 : 1, 정상 : 0\n",
    "\n",
    "    predarray = Cmm.ListToOneValueArraystring(y_pred)\n",
    "\n",
    "    # # calculate AUC of model\n",
    "    # auc = roc_auc_score(y_test, y_pred)\n",
    "    #\n",
    "    # fpr_array = np.array([], dtype=np.int32)\n",
    "    # tpr_array = np.array([], dtype=np.int32)\n",
    "    #\n",
    "    # for i in range(1, 100):\n",
    "    #     predd = np.array([], dtype=np.int32)\n",
    "    #     for j in range(len(pred_prob)):\n",
    "    #         decision_boundary = i / 100\n",
    "    #         if (pred_prob[j][1] > decision_boundary) == True:\n",
    "    #             predd = np.append(predd, 1)\n",
    "    #         else:\n",
    "    #             predd = np.append(predd, 0)\n",
    "    #\n",
    "    #     cofMatt = confusion_matrix(y_test, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    #     ##confusion matrix의 각 요소\n",
    "    #     tpp = cofMatt[1, 1]\n",
    "    #     tnn = cofMatt[0, 0]\n",
    "    #     fnn = cofMatt[1, 0]\n",
    "    #     fpp = cofMatt[0, 1]\n",
    "    #     tpr = tpp / (tpp + fnn)\n",
    "    #     fpr = fpp / (tnn + fpp)\n",
    "    #\n",
    "    #     fpr_array = np.append(fpr_array, fpr)\n",
    "    #     tpr_array = np.append(tpr_array, tpr)\n",
    "    #\n",
    "    # fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    #\n",
    "    # lists = fpr.tolist()\n",
    "    # fprJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = tpr.tolist()\n",
    "    # tprJson = Cmm.NumlistToArraystring(lists)\n",
    "    #\n",
    "    # fpr_array = Cmm.ListToOneValueArraystring(fpr_array)\n",
    "    # tpr_array = Cmm.ListToOneValueArraystring(tpr_array)\n",
    "\n",
    "    # str64 = Cmm.FileToBase64(modelpath)\n",
    "\n",
    "    return {\"Predarray\": predarray}\n",
    "    # return ({\"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\": fpr_array, \"Tprarray\": tpr_array,\n",
    "    #          \"Predarray\": predarray}), {\"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "\n",
    "\n",
    "# a = Classification.knnexcute('SET2022000017-CS','SET2022000017-CT', '계량완료위치,사출최대속도,사출최대압력','판정코드',3)\n",
    "# print(a)\n",
    "\n",
    "# ##예제\n",
    "\n",
    "# train = pd.read_csv(\"C:\\\\Classfication\\\\ab_tr.csv\")\n",
    "# test = pd.read_csv(\"C:\\\\Classfication\\\\ab_te.csv\")\n",
    "#\n",
    "# x_train = train.values[:,0:4]\n",
    "# y_train = train.values[:,4]\n",
    "#\n",
    "# x_test = test.values[:,0:4]\n",
    "# y_test = test.values[:,4]\n",
    "# # #\n",
    "# knn = Classification.knnClassifier(x_train, y_train, x_test, y_test, 3)\n",
    "# print(knn)\n",
    "\n",
    "#knn_model = joblib.load()\n",
    "#print(classification_model_load_KN(knn_model,x_test,y_test))\n",
    "\n",
    "# pickle file load\n",
    "#knnpickle = joblib.load(\"C:\\\\ModelStorage\\\\220718102310792375_CFKnn.pkl\")\n",
    "\n",
    "#print(knnpickle.)\n",
    "\n",
    "\n",
    "#knn_model = joblib.load()\n",
    "#print(classification_model_load_KN(knn_model,x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c214a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import  matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from XNCmmLib import XNCmmUnit as Cmm\n",
    "import XNDatabaseLib as DbLib\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix,f1_score,roc_curve\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# NuralNet (Classification)\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, X_train, hiddenlayer, node, Ouputnode):\n",
    "        super().__init__()  # nn.Module의 __init__속성 및 메소드를 불러옴\n",
    "        self.X_train = X_train\n",
    "        self.hiddenlayer = hiddenlayer\n",
    "        self.node = node\n",
    "        self.Ouputnode = Ouputnode\n",
    "        ordered_dict = OrderedDict()\n",
    "        ordered_dict['Linear1'] = nn.Linear(X_train.size(1), node)\n",
    "        ordered_dict['relu1'] = nn.ReLU()\n",
    "        for i in range(2, hiddenlayer + 2):\n",
    "            ordered_dict['Linear{}'.format(i)] = nn.Linear(node, node)\n",
    "            ordered_dict['relu{}'.format(i)] = nn.ReLU()\n",
    "        ordered_dict['Linear{}'.format(hiddenlayer + 2)] = nn.Linear(node, Ouputnode)\n",
    "        self.layer1 = nn.Sequential(ordered_dict)\n",
    "        # print(self.layer1)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        pred_x = self.layer1(X_test)\n",
    "        pred_x = pred_x.detach().numpy()\n",
    "        pred_x = pred_x.tolist()\n",
    "        pred_x = np.array(pred_x)\n",
    "\n",
    "        return {\"Predarray\" :pred_x}\n",
    "\n",
    "    def fit(self, epochs, loader, criterion, optimizer):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for step, (x, label) in enumerate(loader):\n",
    "                optimizer.zero_grad()  # optimizer의 매개변수를 0으로 만듬\n",
    "                y_pred = self.layer1(x)\n",
    "\n",
    "                loss = criterion(y_pred, label)\n",
    "                losses.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                return\n",
    "            # print(\"{}epoch // loss ={}\".format(epoch, loss))\n",
    "\n",
    "class Classification:\n",
    "    def cnetexcute(trdataID, tsdataID, cols, fcols, hiddenlayer, node, Ouputnode, epochs):\n",
    "        # TrainingID의 cols열을 dataframe 변수\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(trdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        trdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(trdf)\n",
    "        # 예측하는 데이터열을 dataframe 변수\n",
    "        trfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(trfdf)\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(tsdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        tsdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(tsdf)\n",
    "        tsfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(tsfdf)\n",
    "\n",
    "        # cnet 호출\n",
    "        a = Classification.CNet(trdf, trfdf, tsdf, tsfdf, hiddenlayer, node, Ouputnode, epochs)\n",
    "\n",
    "        jstr = Cmm.ClassToJson(a)\n",
    "\n",
    "        return jstr\n",
    "\n",
    "\n",
    "    def CNet(X_train, y_train, X_test, y_test, hiddenlayer, node, Ouputnode, epochs):\n",
    "        '''\n",
    "\n",
    "            @Input\n",
    "                X_train : type : array\n",
    "                y_train : type : array\n",
    "                X_test : type : array\n",
    "                hiddenlayer : type : int\n",
    "                node : type : int\n",
    "                Ouputnode : type : int\n",
    "                epochs : type : int\n",
    "\n",
    "            @Output\n",
    "                pred_y_test : type : array\n",
    "\n",
    "        '''\n",
    "\n",
    "        # 데이터를 파이토치 텐서로 변경\n",
    "        X = torch.Tensor(X_train.values)\n",
    "        Y = torch.Tensor(y_train.values)\n",
    "        X_test = torch.Tensor(X_test.values)\n",
    "\n",
    "        # 학습을 위한 데이터 전처리\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "        # 신경망 생성하기\n",
    "        model = ClassificationNet(X, hiddenlayer, node, Ouputnode)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "        model.fit(epochs, loader, criterion, optimizer)\n",
    "\n",
    "        # 학습되 모델 저장\n",
    "        modelfilename = Cmm.GetSaveModelName(\"CFNeuralnet\")\n",
    "        modelpath = Cmm.GetSaveModelPath() + modelfilename\n",
    "        saveModel = joblib.dump(model, modelpath)\n",
    "        str64 = Cmm.FileToBase64(modelpath)\n",
    "\n",
    "        if os.path.exists(modelpath):\n",
    "            os.remove(modelpath)\n",
    "\n",
    "        # 테스트 데이터로 예측 결과 반환\n",
    "        predarray = model.predict(X_test)\n",
    "        pred_y_test = predarray['Predarray']\n",
    "\n",
    "        #softmax 사용하여 확률 값으로 변환\n",
    "        anomalyScore = np.round(F.softmax(pred_y_test['Predarray'], dim=0).detach().numpy(), 3)\n",
    "\n",
    "        # 파이토치 텐서를 넘파이 어레이로 전환\n",
    "        # pred_y_test = pred_y_test.detach().numpy()\n",
    "        # pred_prob = pred_y_test\n",
    "        #\n",
    "        # fpr_array = np.array([], dtype=np.int32)\n",
    "        # tpr_array = np.array([], dtype=np.int32)\n",
    "        #\n",
    "        # for i in range(1, 100):\n",
    "        #     predd = np.array([], dtype=np.int32)\n",
    "        #     for j in range(len(pred_prob)):\n",
    "        #         decision_boundary = i / 100\n",
    "        #         if (pred_prob[j] > decision_boundary) == True:\n",
    "        #             predd = np.append(predd, 1)\n",
    "        #         else:\n",
    "        #             predd = np.append(predd, 0)\n",
    "        #\n",
    "        #     cofMatt = confusion_matrix(y_test, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "        #     ##confusion matrix의 각 요소\n",
    "        #     tpp = cofMatt[1, 1]\n",
    "        #     tnn = cofMatt[0, 0]\n",
    "        #     fnn = cofMatt[1, 0]\n",
    "        #     fpp = cofMatt[0, 1]\n",
    "        #     tpr = tpp / (tpp + fnn)\n",
    "        #     fpr = fpp / (tnn + fpp)\n",
    "        #\n",
    "        #     fpr_array = np.append(fpr_array, fpr)\n",
    "        #     tpr_array = np.append(tpr_array, tpr)\n",
    "\n",
    "\n",
    "        # calculate AUC of model\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, pred_y_test)\n",
    "        auc = roc_auc_score(y_test, pred_y_test)\n",
    "\n",
    "        lists = fpr.tolist()\n",
    "        fprJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = tpr.tolist()\n",
    "        tprJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = pred_y_test.tolist()\n",
    "        pred_y_testJson = Cmm.NumlistToOneValueArraystring(lists)\n",
    "\n",
    "        return ({\"AnomalyScore\":anomalyScore, \"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson, \"Predarray\" :pred_y_testJson}), {\"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "\n",
    "# trdataid=20220707-0003-CS&tsdataid=20220707-0003-CT&cols=쿠션위치,보압절환위치,계량완료위치,형계위치,사출최대속도&fcols=불량판정&hiddenlayer=2&node=3&ouputnode=5&epochs=10\n",
    "# a = Classification.cnetexcute('SET2022000007-CS','SET2022000007-CT', '계량완료위치,사출최대속도','판정코드',2,5,1,10)\n",
    "# print(a)\n",
    "#\n",
    "#\n",
    "# df1 = pd.read_csv(\"C:\\\\Classfication\\\\ab_te.csv\")\n",
    "# x = df1.iloc[:,[0, 1, 2, 3]]\n",
    "# y = df1.iloc[:,[4]]\n",
    "# X_train,X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=33)\n",
    "# pred_y1_test = Classification.CNet(X_train, y_train, X_test,y_test, 2, 5, 1, 10)\n",
    "# print(pred_y1_test)\n",
    "# pred_y1_test\n",
    "\n",
    "# pred_y1_test['Fpr']\n",
    "# pred_y1_test['Tpr']\n",
    "\n",
    "#\n",
    "#\n",
    "# CNET = torch.load(\"CNet.pt\")\n",
    "# CNET.eval()\n",
    "# X_test = torch.Tensor(X_test.values)\n",
    "# pred_y_test = CNET.predict(X_test)\n",
    "# print(pred_y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a305084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102    0\n",
       "75     1\n",
       "37     1\n",
       "133    0\n",
       "125    0\n",
       "116    0\n",
       "2      1\n",
       "63     1\n",
       "43     1\n",
       "11     1\n",
       "81     1\n",
       "85     1\n",
       "23     1\n",
       "27     1\n",
       "95     1\n",
       "79     1\n",
       "52     1\n",
       "143    0\n",
       "13     1\n",
       "21     1\n",
       "24     1\n",
       "57     1\n",
       "41     1\n",
       "68     1\n",
       "111    0\n",
       "18     1\n",
       "122    0\n",
       "104    0\n",
       "74     1\n",
       "132    0\n",
       "56     1\n",
       "25     1\n",
       "39     1\n",
       "106    0\n",
       "69     1\n",
       "145    0\n",
       "91     1\n",
       "78     1\n",
       "96     1\n",
       "131    0\n",
       "7      1\n",
       "128    0\n",
       "134    0\n",
       "127    0\n",
       "147    0\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49633afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from XNCmmLib import XNCmmUnit as Cmm\n",
    "import XNDatabaseLib as DbLib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score, f1_score,roc_curve\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "class Classification:\n",
    "    def randomforestexcute(trdataID, tsdataID, cols, fcols, criterion, max_depth, n_estimators):\n",
    "        # TrainingID를 dataframe 변수\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(trdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        trdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(trdf)\n",
    "\n",
    "        # 불량판정 데이터열을 dataframe 변수\n",
    "        trfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(trfdf)\n",
    "\n",
    "        # 테스트할 ID 값을 dataframe 변수\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(tsdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        tsdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(tsdf)\n",
    "\n",
    "        tsfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(tsfdf)\n",
    "\n",
    "        # cbm 호출\n",
    "        a = Classification.classification_randomforest(trdf, tsdf ,trfdf, tsfdf, criterion, max_depth, n_estimators)\n",
    "        jstr = Cmm.ClassToJson(a)\n",
    "\n",
    "        return jstr\n",
    "\n",
    "    def classification_randomforest(trainX, testX, trainY, testY,criterion='gini', max_depth=None, n_estimators=100 ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainX : array\n",
    "            Train X.\n",
    "        testX : array\n",
    "            Test X.\n",
    "        trainY : array\n",
    "            Train Y. 이진(binary) 범주형 데이터\n",
    "        testY : array\n",
    "            Test Y. 이진(binary) 범저형 데이터\n",
    "        n_estimators : int, optional, default = 100\n",
    "            의사 결정 나무의 갯수\n",
    "        max_depth : int, optional, default = None\n",
    "            의사 결정 나무의 최대 깊이. 의사 결정 나무가 최대 깊이에 도달하면 더 이상 분기하지 않음.\n",
    "            None 입력 시 각 노드에 1개의 샘플이 남을 때까지 분기함\n",
    "        criterion : {'gini', 'entropy', 'log_loss'}, default = \"gini\"\n",
    "            의사 결정 나무의 분기 기준.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model :\n",
    "            RandomForest 모델.\n",
    "        confusionMatrix : array\n",
    "            분류 성능 평가를 위한 Confusion Matrix\n",
    "        fpr : array\n",
    "            False Positive Rate\n",
    "        tpr : array\n",
    "            True Positive Rate\n",
    "        auc_score : float\n",
    "            fpr, tpr의 AUC(Area Under Cruve) scroe (0~1). 1에 가까울 수록 분류 성능이 좋음을 의미함\n",
    "        accuracy : float\n",
    "            accuracy.\n",
    "        precision : float\n",
    "            precision\n",
    "        recall : float\n",
    "            recall\n",
    "        f1 : float\n",
    "            f1 score\n",
    "        tp : int\n",
    "            True positive\n",
    "        fp : int\n",
    "            False positive\n",
    "        fn : int\n",
    "            False negative\n",
    "        tn : int\n",
    "            True negative\n",
    "\n",
    "        \"\"\"\n",
    "        # 데이터가 DataFrame이면 array타입으로 변경\n",
    "        if isinstance(trainX, pd.DataFrame):\n",
    "            trainX = trainX.values\n",
    "        if isinstance(testX, pd.DataFrame):\n",
    "            testX = testX.values\n",
    "        if isinstance(trainY, pd.DataFrame):\n",
    "            trainY = trainY.values\n",
    "        if isinstance(testY, pd.DataFrame):\n",
    "            testY = testY.values\n",
    "\n",
    "        # 모델링\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion=criterion)\n",
    "        model.fit(trainX, trainY)\n",
    "\n",
    "        # Random Forest 모델 Pickle 파일로 저장\n",
    "        modelfilename = Cmm.GetSaveModelName(\"CFRandomforest\")\n",
    "        modelpath = Cmm.GetSaveModelPath() + modelfilename\n",
    "        saveModel = joblib.dump(model, modelpath)\n",
    "        str64 = Cmm.FileToBase64(modelpath)\n",
    "\n",
    "        if os.path.exists(modelpath):\n",
    "            os.remove(modelpath)\n",
    "\n",
    "        # 예측 값\n",
    "        pred = model.predict(testX)\n",
    "        anomalyScore = model.predict_proba(testX)\n",
    "\n",
    "        predarray = Cmm.ListToOneValueArraystring(pred)\n",
    "\n",
    "        # 분류 평가 지표\n",
    "        cofMat = confusion_matrix(testY, pred, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "        # confusion matrix의 각 요소\n",
    "        tp = cofMat[1, 1]\n",
    "        tn = cofMat[0, 0]\n",
    "        fn = cofMat[1, 0]\n",
    "        fp = cofMat[0, 1]\n",
    "\n",
    "        accuracy = accuracy_score(testY, pred)\n",
    "        precision = precision_score(testY, pred)\n",
    "        recall = recall_score(testY, pred)\n",
    "        f1 = f1_score(testY, pred)\n",
    "\n",
    "        # calculate AUC of model\n",
    "        auc = roc_auc_score(testY, pred)\n",
    "        #fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "\n",
    "        fpr_array = np.array([], dtype=np.int32)\n",
    "        tpr_array = np.array([], dtype=np.int32)\n",
    "\n",
    "        for i in range(1, 100):\n",
    "            predd = np.array([], dtype=np.int32)\n",
    "            for j in range(len(anomalyScore)):\n",
    "                decision_boundary = i / 100\n",
    "                if (anomalyScore[j][1] > decision_boundary) == True:\n",
    "                    predd = np.append(predd, 1)\n",
    "                else:\n",
    "                    predd = np.append(predd, 0)\n",
    "\n",
    "            cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "            ##confusion matrix의 각 요소\n",
    "            tpp = cofMatt[1, 1]\n",
    "            tnn = cofMatt[0, 0]\n",
    "            fnn = cofMatt[1, 0]\n",
    "            fpp = cofMatt[0, 1]\n",
    "            tpr = tpp / (tpp + fnn)\n",
    "            fpr = fpp / (tnn + fpp)\n",
    "\n",
    "            fpr_array = np.append(fpr_array, fpr)\n",
    "            tpr_array = np.append(tpr_array, tpr)\n",
    "\n",
    "        fpr_arrayJson = Cmm.ListToOneValueArraystring(fpr_array)\n",
    "        tpr_arrayJson = Cmm.ListToOneValueArraystring(tpr_array)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "\n",
    "        lists = cofMat.tolist()\n",
    "        cofMatJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = fpr.tolist()\n",
    "        fprJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = tpr.tolist()\n",
    "        tprJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        anomalyScore = np.round(anomalyScore[:, 1], 3)\n",
    "        lists = anomalyScore.tolist()\n",
    "        anomalyScoreJson = Cmm.NumlistToArraystring(lists)\n",
    "        # print(anomalyScoreJson)\n",
    "\n",
    "        return ({\"AnomalyScore\":anomalyScoreJson, \"Aucscore\": str(auc),\"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\": fpr_arrayJson, \"Tprarray\": tpr_arrayJson, \"Predarray\" : predarray}), {\"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "                # \"model\": model, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "                # \"tn\": tn})  # type : int32\n",
    "\n",
    "def classification_model_load_rf(model, testX):\n",
    "    pred = model.predict(testX)\n",
    "    anomalyScore = model.predict_proba(testX)\n",
    "    predarray = Cmm.ListToOneValueArraystring(pred)\n",
    "\n",
    "    # 분류 평가 지표\n",
    "    # cofMat = confusion_matrix(testY, pred, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    # # confusion matrix의 각 요소\n",
    "    # tp = cofMat[1, 1]\n",
    "    # tn = cofMat[0, 0]\n",
    "    # fn = cofMat[1, 0]\n",
    "    # fp = cofMat[0, 1]\n",
    "    #\n",
    "    # accuracy = accuracy_score(testY, pred)\n",
    "    # precision = precision_score(testY, pred)\n",
    "    # recall = recall_score(testY, pred)\n",
    "    # f1 = f1_score(testY, pred)\n",
    "    #\n",
    "    # # calculate AUC of model\n",
    "    # auc = roc_auc_score(testY, pred)\n",
    "    # # fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "    #\n",
    "    # fpr_array = np.array([], dtype=np.int32)\n",
    "    # tpr_array = np.array([], dtype=np.int32)\n",
    "    #\n",
    "    # for i in range(1, 100):\n",
    "    #     predd = np.array([], dtype=np.int32)\n",
    "    #     for j in range(len(pred_prob)):\n",
    "    #         decision_boundary = i / 100\n",
    "    #         if (pred_prob[j][1] > decision_boundary) == True:\n",
    "    #             predd = np.append(predd, 1)\n",
    "    #         else:\n",
    "    #             predd = np.append(predd, 0)\n",
    "    #\n",
    "    #     cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    #     ##confusion matrix의 각 요소\n",
    "    #     tpp = cofMatt[1, 1]\n",
    "    #     tnn = cofMatt[0, 0]\n",
    "    #     fnn = cofMatt[1, 0]\n",
    "    #     fpp = cofMatt[0, 1]\n",
    "    #     tpr = tpp / (tpp + fnn)\n",
    "    #     fpr = fpp / (tnn + fpp)\n",
    "    #\n",
    "    #     fpr_array = np.append(fpr_array, fpr)\n",
    "    #     tpr_array = np.append(tpr_array, tpr)\n",
    "    #\n",
    "    # fpr_arrayJson = Cmm.ListToOneValueArraystring(fpr_array)\n",
    "    # tpr_arrayJson = Cmm.ListToOneValueArraystring(tpr_array)\n",
    "    #\n",
    "    # fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "\n",
    "    # lists = cofMat.tolist()\n",
    "    # cofMatJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = fpr.tolist()\n",
    "    # fprJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = tpr.tolist()\n",
    "    # tprJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "    return {\"Predarray\": predarray}\n",
    "    # return ({\"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\": fpr_arrayJson, \"Tprarray\": tpr_arrayJson,\n",
    "    #          \"Predarray\": predarray})#, {\"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "    # \"model\": model, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "    # \"tn\": tn})  # type : int32\n",
    "\n",
    "#\n",
    "# a = Classification.randomforestexcute('SET2022000017-CS','SET2022000017-CT', '계량완료위치,사출최대속도,사출최대압력','판정코드','gini', None, 100)\n",
    "# print(a)\n",
    "\n",
    "# ##예제\n",
    "# ab_tr = pd.read_csv('C:\\\\Classfication\\\\ab_tr.csv')\n",
    "# ab_te = pd.read_csv('C:\\\\Classfication\\\\ab_te.csv')\n",
    "#\n",
    "# # ab_tr = ab_tr.drop(['Unnamed: 0'],axis=1)\n",
    "# # ab_te = ab_te.drop(['Unnamed: 0'],axis=1)\n",
    "#\n",
    "# ab_tr_x = ab_tr.drop(['target'],axis=1)\n",
    "# ab_te_x = ab_te.drop(['target'],axis=1)\n",
    "# # print(ab_tr_x)\n",
    "# print(ab_te_x)\n",
    "# ab_tr_y = ab_tr['target']\n",
    "# ab_te_y = ab_te['target']\n",
    "# # print(ab_tr_y)\n",
    "# print(ab_te_y)\n",
    "\n",
    "# ab_dt = Classification.classification_randomforest(ab_tr_x, ab_te_x, ab_tr_y, ab_te_y)\n",
    "# print(ab_dt)\n",
    "\n",
    "#rf_model = joblib.load()\n",
    "#print(classification_model_load_KN(rf_model,ab_te_x,ab_te_y))\n",
    "\n",
    "# rf_model = joblib.load('C:\\\\ModelStorage\\\\220718174100297622_CFRandomforest.pkl')\n",
    "# print(classification_model_load_KN(rf_model,,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71bb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix,f1_score,roc_curve\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from XNCmmLib import XNCmmUnit as Cmm\n",
    "import XNDatabaseLib as DbLib\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "class Classification:\n",
    "    def svmexcute(trdataID, tsdataID, cols, fcols):\n",
    "        # TrainingID의 cols열을 dataframe 변수\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(trdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        trdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(trdf)\n",
    "        # 예측하는 데이터열을 dataframe 변수\n",
    "        trfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(trfdf)\n",
    "        txt = DbLib.DatabaseUnit.GetDataSet(tsdataID)\n",
    "        df = pd.read_csv(Cmm.StrToStringIO(txt), sep=',', encoding='utf-8-sig')\n",
    "        tsdf = Cmm.DfToCustomDf(df, Cmm.StrToList(cols, ','))\n",
    "        # print(tsdf)\n",
    "        tsfdf = Cmm.DfToCustomDf(df, Cmm.StrToList(fcols, ','))\n",
    "        # print(tsfdf)\n",
    "        # cbm 호출\n",
    "        a = Classification.svm_classification(trdf, trfdf ,tsdf, tsfdf)\n",
    "        jstr = Cmm.ClassToJson(a)\n",
    "\n",
    "        return jstr\n",
    "\n",
    "    def svm_classification(trainX, trainY, testX, testY):\n",
    "        if isinstance(trainX, pd.DataFrame):\n",
    "            trainX = trainX.values\n",
    "        if isinstance(testX, pd.DataFrame):\n",
    "            testX = testX.values\n",
    "        if isinstance(trainY, pd.DataFrame):\n",
    "            trainY = trainY.values\n",
    "        if isinstance(testY, pd.DataFrame):\n",
    "            testY = testY.values\n",
    "\n",
    "        model = svm.SVC(kernel='rbf',probability=True).fit(trainX, trainY)\n",
    "\n",
    "        # SVM 모델 Pickle 파일로 저장\n",
    "        modelfilename = Cmm.GetSaveModelName(\"CFSvm\")\n",
    "        modelpath = Cmm.GetSaveModelPath() + modelfilename\n",
    "        saveModel = joblib.dump(model, modelpath)\n",
    "        str64 = Cmm.FileToBase64(modelpath)\n",
    "\n",
    "        if os.path.exists(modelpath):\n",
    "            os.remove(modelpath)\n",
    "\n",
    "        anomalyScore = model.predict_proba(testX)\n",
    "        pred = model.predict(testX)\n",
    "        predarray = Cmm.ListToOneValueArraystring(pred)\n",
    "\n",
    "        fpr_array = np.array([], dtype=np.int32)\n",
    "        tpr_array = np.array([], dtype=np.int32)\n",
    "\n",
    "        for i in range(1, 100):\n",
    "            predd = np.array([], dtype=np.int32)\n",
    "            for j in range(len(anomalyScore)):\n",
    "                decision_boundary = i / 100\n",
    "                if (anomalyScore[j][1] > decision_boundary) == True:\n",
    "                    predd = np.append(predd, 1)\n",
    "                else:\n",
    "                    predd = np.append(predd, 0)\n",
    "\n",
    "            cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "            ##confusion matrix의 각 요소\n",
    "            tpp = cofMatt[1, 1]\n",
    "            tnn = cofMatt[0, 0]\n",
    "            fnn = cofMatt[1, 0]\n",
    "            fpp = cofMatt[0, 1]\n",
    "            tpr = tpp / (tpp + fnn)\n",
    "            fpr = fpp / (tnn + fpp)\n",
    "\n",
    "            fpr_array = np.append(fpr_array, fpr)\n",
    "            tpr_array = np.append(tpr_array, tpr)\n",
    "\n",
    "        #fpr, tpr, threshold = roc_curve(testY, pred)\n",
    "        auc = roc_auc_score(testY, pred)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "\n",
    "        lists = fpr.tolist()\n",
    "        fprJson = Cmm.NumlistToArraystring(lists)\n",
    "        lists = tpr.tolist()\n",
    "        tprJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        anomalyScore = np.round(anomalyScore[:, 1],3)\n",
    "        lists = anomalyScore.tolist()\n",
    "        anomalyScoreJson = Cmm.NumlistToArraystring(lists)\n",
    "\n",
    "        fpr_arrayJson = Cmm.ListToOneValueArraystring(fpr_array)\n",
    "        tpr_arrayJson = Cmm.ListToOneValueArraystring(tpr_array)\n",
    "\n",
    "        return ({\"AnomalyScore\":anomalyScoreJson, \"Aucscore\": str(auc),\"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\": fpr_arrayJson, \"Tprarray\": tpr_arrayJson, \"Predarray\" : predarray}), {\"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "\n",
    "def classification_model_load_svm(model, testX):\n",
    "    anomalyScore = model.predict_proba(testX)\n",
    "    pred = model.predict(testX)\n",
    "    predarray = Cmm.ListToOneValueArraystring(pred)\n",
    "\n",
    "    # fpr_array = np.array([], dtype=np.int32)\n",
    "    # tpr_array = np.array([], dtype=np.int32)\n",
    "    #\n",
    "    # for i in range(1, 100):\n",
    "    #     predd = np.array([], dtype=np.int32)\n",
    "    #     for j in range(len(pred_prob)):\n",
    "    #         decision_boundary = i / 100\n",
    "    #         if (pred_prob[j][1] > decision_boundary) == True:\n",
    "    #             predd = np.append(predd, 1)\n",
    "    #         else:\n",
    "    #             predd = np.append(predd, 0)\n",
    "    #\n",
    "    #     cofMatt = confusion_matrix(testY, predd, labels=[1, 0])  # 1: 불량, 0: 정상\n",
    "    #     ##confusion matrix의 각 요소\n",
    "    #     tpp = cofMatt[1, 1]\n",
    "    #     tnn = cofMatt[0, 0]\n",
    "    #     fnn = cofMatt[1, 0]\n",
    "    #     fpp = cofMatt[0, 1]\n",
    "    #     tpr = tpp / (tpp + fnn)\n",
    "    #     fpr = fpp / (tnn + fpp)\n",
    "    #\n",
    "    #     fpr_array = np.append(fpr_array, fpr)\n",
    "    #     tpr_array = np.append(tpr_array, tpr)\n",
    "    #\n",
    "    # # fpr, tpr, threshold = roc_curve(testY, pred)\n",
    "    # auc = roc_auc_score(testY, pred)\n",
    "    #\n",
    "    # fpr, tpr, thresholds = roc_curve(testY, pred)\n",
    "    #\n",
    "    # lists = fpr.tolist()\n",
    "    # fprJson = Cmm.NumlistToArraystring(lists)\n",
    "    # lists = tpr.tolist()\n",
    "    # tprJson = Cmm.NumlistToArraystring(lists)\n",
    "    #\n",
    "    # fpr_arrayJson = Cmm.ListToOneValueArraystring(fpr_array)\n",
    "    # tpr_arrayJson = Cmm.ListToOneValueArraystring(tpr_array)\n",
    "\n",
    "    return {\"Predarray\": predarray}\n",
    "    # return ({\"Aucscore\": str(auc), \"Fpr\": fprJson, \"Tpr\": tprJson, \"Fprarray\": fpr_arrayJson, \"Tprarray\": tpr_arrayJson,\n",
    "    #          \"Predarray\": predarray})#, {\"Modelfile\": str(modelfilename), \"Modelcontents\": str(str64)}\n",
    "\n",
    "# classification-svm?trdataid=20220707-0003-CS&tsdataid=20220707-0003-CT&cols=계량완료위치,쿠션위치,보압절환위치&fcols=불량판정\n",
    "# a = Classification.svmexcute('SET2022000017-CS','SET2022000017-CT', '계량완료위치,사출최대속도,사출최대압력','판정코드')\n",
    "# print(a)\n",
    "\n",
    "# ## 예제\n",
    "# ab_tr = pd.read_csv('C:\\\\Classfication\\\\ab_tr.csv')\n",
    "# ab_te = pd.read_csv('C:\\\\Classfication\\\\ab_te.csv')\n",
    "#\n",
    "#\n",
    "# trainX = ab_tr.drop(['target'],axis=1)\n",
    "# trainY = ab_tr['target']\n",
    "#\n",
    "# testX = ab_te.drop(['target'],axis=1)\n",
    "# testY = ab_te['target']\n",
    "#\n",
    "# ab_svm = Classification.svm_classification(trainX,trainY,testX,testY)\n",
    "# print(ab_svm)\n",
    "\n",
    "\n",
    "#svm_model = joblib.load()\n",
    "#print(classification_model_load_KN(svm_model,testX,testY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
